Dependency Taxonomy,Configuration Parameter A,Configuration Parameter B,Class,Function
Control Dependency,hbase.data.umask.enable,mapreduce.application.framework.path,org.apache.hadoop.hbase.regionserver.HRegionFileSystem,"<org.apache.hadoop.hbase.regionserver.HRegionFileSystem: boolean mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>"
Control Dependency,dfs.namenode.servicerpc-address,hbase.regionserver.info.port,org.apache.hadoop.hbase.master.HMaster,<org.apache.hadoop.hbase.master.HMaster: int getRegionServerInfoPort(org.apache.hadoop.hbase.ServerName)>
Control Dependency,hadoop.security.authentication,hbase.security.authentication,org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager,<org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager: org.apache.hadoop.hbase.security.User getActiveUser()>
Control Dependency,fs.defaultFS,hbase.data.umask.enable,org.apache.hadoop.hbase.regionserver.HRegionFileSystem,"<org.apache.hadoop.hbase.regionserver.HRegionFileSystem: boolean mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>"
Control Dependency,dfs.client.read.shortcircuit.buffer.size,hbase.dfs.client.read.shortcircuit.buffer.size,org.apache.hadoop.hbase.util.CommonFSUtils,<org.apache.hadoop.hbase.util.CommonFSUtils: void checkShortCircuitReadBufferSize(org.apache.hadoop.conf.Configuration)>
Control Dependency,spark.executor.id,spark.driver.port,org.apache.spark.util.RpcUtils$,"<org.apache.spark.util.RpcUtils$: org.apache.spark.rpc.RpcEndpointRef makeDriverRef(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEnv)>"
Control Dependency,spark.executor.id,spark.driver.host,org.apache.spark.util.RpcUtils$,"<org.apache.spark.util.RpcUtils$: org.apache.spark.rpc.RpcEndpointRef makeDriverRef(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEnv)>"
Control Dependency,spark.deploy.recoveryMode,spark.deploy.recoveryMode.factory,org.apache.spark.deploy.master.Master,<org.apache.spark.deploy.master.Master: void onStart()>
Control Dependency,spark.authenticate,spark.master,org.apache.spark.SecurityManager,<org.apache.spark.SecurityManager: void initializeAuth()>
Control Dependency,alluxio.worker.data.folder,alluxio.PropertyKey,alluxio.util.io.PathUtils,<alluxio.util.io.PathUtils: java.lang.String getWorkerDataDirectory(java.lang.String)>
Control Dependency,hadoop.security.crypto.cipher.suite,mapreduce.job.encrypted-intermediate-data,org.apache.hadoop.crypto.CryptoCodec,<org.apache.hadoop.crypto.CryptoCodec: org.apache.hadoop.crypto.CryptoCodec getInstance(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.job.emit-timeline-data,yarn.timeline-service.enabled,org.apache.hadoop.yarn.conf.YarnConfiguration,<org.apache.hadoop.yarn.conf.YarnConfiguration: boolean timelineServiceEnabled(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.fileoutputcommitter.algorithm.version,spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,*,*
Control Dependency,mapred.reducer.class,mapreduce.job.reduces,org.apache.hadoop.mapreduce.Job,<org.apache.hadoop.mapreduce.Job: void setUseNewAPI()>
Control Dependency,mapreduce.job.sharedcache.mode,yarn.sharedcache.enabled,org.apache.hadoop.mapreduce.SharedCacheConfig,<org.apache.hadoop.mapreduce.SharedCacheConfig: void init(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.framework.name,yarn.sharedcache.enabled,org.apache.hadoop.mapreduce.SharedCacheConfig,<org.apache.hadoop.mapreduce.SharedCacheConfig: void init(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.job.shuffle.provider.services,yarn.nodemanager.aux-services,org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: void addExternalShuffleProviders(org.apache.hadoop.conf.Configuration,java.util.Map)>"
Control Dependency,hadoop.security.authentication,mapreduce.jobhistory.keytab,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: void login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>"
Control Dependency,mapred.mapper.new-api,mapreduce.job.reduces,org.apache.hadoop.mapreduce.v2.app.MRAppMaster,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void serviceInit(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapreduce.job.am-access-disabled,yarn.app.mapreduce.client.max-retries,org.apache.hadoop.mapred.ClientServiceDelegate,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
Control Dependency,mapreduce.jobhistory.maximum-start-wait-time-millis,yarn.app.mapreduce.am.staging-dir,org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils,<org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: java.lang.String getConfiguredHistoryIntermediateDoneDirPrefix(org.apache.hadoop.conf.Configuration)>
Control Dependency,mapred.reducer.new-api,mapreduce.job.reduces,org.apache.hadoop.mapreduce.v2.app.MRAppMaster,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void serviceInit(org.apache.hadoop.conf.Configuration)>
Control Dependency,hadoop.security.authentication,yarn.nodemanager.keytab,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: void login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>"
Control Dependency,dfs.journalnode.keytab.file,hadoop.security.authentication,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: void login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>"
Control Dependency,dfs.datanode.dns.nameserver,hadoop.security.dns.nameserver,org.apache.hadoop.net.DNS,"<org.apache.hadoop.net.DNS: java.lang.String getDefaultHost(java.lang.String,java.lang.String,boolean)>"
Control Dependency,dfs.namenode.kerberos.principal,hadoop.security.authentication,org.apache.hadoop.hdfs.server.namenode.NameNode,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean format(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Control Dependency,fs.defaultFS,hadoop.shell.missing.defaultFs.warning,org.apache.hadoop.fs.shell.FsCommand,<org.apache.hadoop.fs.shell.FsCommand: void processRawArguments(java.util.LinkedList)>
Control Dependency,dfs.journalnode.kerberos.principal,hadoop.security.authentication,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: void login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>"
Control Dependency,dfs.ha.automatic-failover.enable,ha.zookeeper.session-timeout.ms,org.apache.hadoop.ha.ZKFailoverController,*
Control Dependency,hadoop.security.authentication,yarn.web-proxy.principal,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: void login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>"
Control Dependency,hadoop.security.authentication,yarn.web-proxy.keytab,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: void login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>"
Control Dependency,fs.client.resolve.topology.enabled,net.topology.node.switch.mapping.impl,org.apache.hadoop.hdfs.ClientContext,<org.apache.hadoop.hdfs.ClientContext: void initTopologyResolution(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.web.ugi,hadoop.http.staticuser.user,org.apache.hadoop.http.lib.StaticUserWebFilter,<org.apache.hadoop.http.lib.StaticUserWebFilter: java.lang.String getUsernameFromConf(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.namenode.keytab.file,hadoop.security.authentication,org.apache.hadoop.hdfs.server.namenode.NameNode,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean format(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Control Dependency,dfs.secondary.namenode.kerberos.principal,hadoop.security.authentication,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode,"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>"
Control Dependency,hadoop.security.authentication,yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil: void setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager)>"
Control Dependency,dfs.datanode.dns.interface,hadoop.security.dns.interface,org.apache.hadoop.hdfs.server.datanode.DataNode,<org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.String getHostName(org.apache.hadoop.conf.Configuration)>
Control Dependency,hadoop.security.authentication,io.file.buffer.size,org.apache.hadoop.fs.FileSystem,<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path)>
Control Dependency,dfs.http.policy,hadoop.ssl.enabled,org.apache.hadoop.hdfs.DFSUtil,<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpConfig$Policy getHttpPolicy(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.ha.automatic-failover.enable,ha.zookeeper.auth,org.apache.hadoop.ha.ZKFailoverController,*
Control Dependency,dfs.ha.automatic-failover.enable,ha.zookeeper.acl,org.apache.hadoop.ha.ZKFailoverController,*
Control Dependency,dfs.ha.automatic-failover.enable,hadoop.security.authorization,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: void login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>"
Control Dependency,hadoop.security.authentication,yarn.resourcemanager.keytab,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: void login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>"
Control Dependency,io.native.lib.available,zlib.compress.level,org.apache.hadoop.io.compress.zlib.ZlibFactory,<org.apache.hadoop.io.compress.zlib.ZlibFactory: org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel getCompressionLevel(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.ha.automatic-failover.enable,ha.zookeeper.quorum,org.apache.hadoop.ha.ZKFailoverController,*
Control Dependency,dfs.datanode.keytab.file,hadoop.security.authentication,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: void login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)>"
Control Dependency,dfs.ha.automatic-failover.enable,ha.zookeeper.parent-znode,org.apache.hadoop.ha.ZKFailoverController,*
Control Dependency,dfs.datanode.keytab.file,hadoop.security.authorization,org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService,<org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: void serviceStart()>
Control Dependency,dfs.datanode.keytab.file,yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime,<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: boolean allowPrivilegedContainerExecution(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)>
Control Dependency,hadoop.http.filter.initializers,yarn.timeline-service.http-cross-origin.enabled,org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer,<org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer: void startWebApp()>
Control Dependency,ha.failover-controller.cli-check.rpc-timeout.ms,yarn.resourcemanager.ha.rm-ids,org.apache.hadoop.yarn.util.RMHAUtils,<org.apache.hadoop.yarn.util.RMHAUtils: org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getHAState(org.apache.hadoop.yarn.conf.YarnConfiguration)>
Control Dependency,ha.failover-controller.cli-check.rpc-timeout.ms,yarn.resourcemanager.webapp.cross-origin.enabled,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil: void setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager)>"
Control Dependency,electionType,zookeeper.cnxTimeout,org.apache.zookeeper.server.quorum.QuorumCnxManager,*
Control Dependency,dfs.namenode.rpc-address,fs.defaultFS,org.apache.hadoop.fs.FileSystem,<org.apache.hadoop.fs.FileSystem: java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration)>
Control Dependency,dfs.block.access.token.enable,hadoop.security.authentication,org.apache.hadoop.hdfs.server.datanode.DataNode,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkSecureConfig(org.apache.hadoop.hdfs.server.datanode.DNConf,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Control Dependency,dfs.namenode.secondary.http-address,hadoop.security.authentication,org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,hadoop.security.authentication,hbase.thrift.security.qop,org.apache.hadoop.hbase.thrift.ThriftServerRunner,*
Value Relationship Dependency,dfs.encrypt.data.transfer.cipher.key.bitlength,hbase.hstore.bytes.per.checksum,org.apache.hadoop.hdfs.RemoteBlockReader,"<org.apache.hadoop.hdfs.RemoteBlockReader: int readChunkImpl(long,byte[],int,int,byte[])>"
Value Relationship Dependency,spark.log.callerContext,hadoop.caller.context.max.size,org.apache.spark.util.CallerContext,<org.apache.spark.util.CallerContext: java.lang.String prepareContext(java.lang.String)>
Value Relationship Dependency,spark.shuffle.service.enabled,spark.dynamicAllocation.enabled,org.apache.spark.util.Utils$,<org.apache.spark.util.Utils$: int getDynamicAllocationInitialExecutors(org.apache.spark.SparkConf)>
Value Relationship Dependency,mapreduce.map.cpu.vcores,yarn.app.mapreduce.am.resource.cpu-vcores,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Value Relationship Dependency,dfs.blocksize,mapreduce.input.fileinputformat.split.maxsize,org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat$OneFileInfo,"<org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat$OneFileInfo: void <init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.conf.Configuration,boolean,java.util.HashMap,java.util.HashMap,java.util.HashMap,java.util.HashMap,long)>"
Value Relationship Dependency,dfs.blocksize,mapreduce.input.fileinputformat.split.minsize,org.apache.hadoop.mapreduce.lib.input.FileInputFormat,<org.apache.hadoop.mapreduce.lib.input.FileInputFormat: long computeSplitSize()>
Value Relationship Dependency,mapreduce.map.memory.mb,yarn.app.mapreduce.am.resource.mb,org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
Value Relationship Dependency,mapreduce.job.shuffle.provider.services,yarn.nodemanager.aux-services,org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: void addExternalShuffleProviders(org.apache.hadoop.conf.Configuration,java.util.Map)>"
Value Relationship Dependency,hadoop.http.cross-origin.enabled,yarn.resourcemanager.webapp.cross-origin.enabled,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil: void setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager)>"
Value Relationship Dependency,dfs.web.authentication.kerberos.keytab,hadoop.security.authentication,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,*
Value Relationship Dependency,dfs.block.access.token.enable,hadoop.security.authentication,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,*
Value Relationship Dependency,dfs.image.compression.codec,io.compression.codecs,org.apache.hadoop.io.compress.CompressionCodecFactory,<org.apache.hadoop.io.compress.CompressionCodecFactory: org.apache.hadoop.io.compress.CompressionCodec getCodec(org.apache.hadoop.fs.Path)>
Value Relationship Dependency,hadoop.http.cross-origin.enabled,yarn.timeline-service.http-cross-origin.enabled,org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer,<org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer: void addFilters(org.apache.hadoop.conf.Configuration)>
Value Relationship Dependency,dfs.web.authentication.kerberos.principal,hadoop.security.authentication,org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer,*
Value Relationship Dependency,hadoop.http.cross-origin.enabled,yarn.nodemanager.webapp.cross-origin.enabled,org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer,<org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer: void serviceStart()>
Value Relationship Dependency,dfs.webhdfs.acl.provider.permission.pattern,fs.defaultFS,org.apache.hadoop.hdfs.web.resources.StringParam$Domain,<org.apache.hadoop.hdfs.web.resources.StringParam$Domain: java.lang.String parse(java.lang.String)>
Value Relationship Dependency,dfs.internal.nameservices,fs.protected.directories,org.apache.hadoop.hdfs.server.mover.Mover$Cli,"<org.apache.hadoop.hdfs.server.mover.Mover$Cli: java.util.Map getNameNodePaths(org.apache.commons.cli.CommandLine,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,dfs.nameservices,fs.protected.directories,org.apache.hadoop.hdfs.server.mover.Mover$Cli,"<org.apache.hadoop.hdfs.server.mover.Mover$Cli: java.util.Map getNameNodePaths(org.apache.commons.cli.CommandLine,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,dfs.client.read.shortcircuit.buffer.size,io.file.buffer.size,org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy,"<org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy: int getSlowReadBufferNumChunks(int,int)>"
Value Relationship Dependency,dfs.webhdfs.acl.provider.permission.pattern,fs.protected.directories,org.apache.hadoop.hdfs.web.resources.StringParam$Domain,<org.apache.hadoop.hdfs.web.resources.StringParam$Domain: java.lang.String parse(java.lang.String)>
Value Relationship Dependency,dfs.client.socket-timeout,hadoop.security.crypto.buffer.size,org.apache.hadoop.crypto.CryptoOutputStream,<org.apache.hadoop.crypto.CryptoOutputStream: void encrypt()>
Value Relationship Dependency,dfs.cluster.administrators,hadoop.security.groups.cache.secs,org.apache.hadoop.security.authorize.AccessControlList,<org.apache.hadoop.security.authorize.AccessControlList: boolean isUserInList(org.apache.hadoop.security.UserGroupInformation)>
Value Relationship Dependency,dfs.client-write-packet-size,io.file.buffer.size,org.apache.hadoop.hdfs.DFSOutputStream,<org.apache.hadoop.hdfs.DFSOutputStream: void adjustChunkBoundary()>
Value Relationship Dependency,dfs.block.local-path-access.user,hadoop.http.staticuser.user,org.apache.hadoop.hdfs.server.datanode.DataNode,<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkBlockLocalPathAccess()>
Value Relationship Dependency,dfs.cluster.administrators,hadoop.http.staticuser.user,org.apache.hadoop.security.authorize.AccessControlList,<org.apache.hadoop.security.authorize.AccessControlList: boolean isUserInList(org.apache.hadoop.security.UserGroupInformation)>
Value Relationship Dependency,dfs.block.scanner.volume.bytes.per.second,io.file.buffer.size,org.apache.hadoop.hdfs.server.datanode.VolumeScanner,"<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: boolean calculateShouldScan(java.lang.String,long,long,long,long)>"
Value Relationship Dependency,dfs.datanode.readahead.bytes,io.file.buffer.size,org.apache.hadoop.io.ReadaheadPool,"<org.apache.hadoop.io.ReadaheadPool: org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)>"
Value Relationship Dependency,dfs.namenode.fs-limits.max-xattr-size,io.file.buffer.size,org.apache.hadoop.hdfs.server.namenode.TransferFsImage,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash receiveFile(java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean,long,org.apache.hadoop.io.MD5Hash,java.lang.String,java.io.InputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
Value Relationship Dependency,dfs.datanode.address,io.file.buffer.size,org.apache.hadoop.hdfs.protocolPB.PBHelperClient,<org.apache.hadoop.hdfs.protocolPB.PBHelperClient: java.io.InputStream vintPrefixed(java.io.InputStream)>
Value Relationship Dependency,dfs.client.cache.readahead,io.file.buffer.size,org.apache.hadoop.io.ReadaheadPool,"<org.apache.hadoop.io.ReadaheadPool: org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)>"
Value Relationship Dependency,dfs.namenode.checkpoint.dir,fs.protected.directories,org.apache.hadoop.hdfs.server.datanode.DataStorage,<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeVolumes(java.util.Set)>
Value Relationship Dependency,dfs.webhdfs.acl.provider.permission.pattern,hadoop.http.staticuser.user,org.apache.hadoop.hdfs.web.resources.StringParam$Domain,<org.apache.hadoop.hdfs.web.resources.StringParam$Domain: java.lang.String parse(java.lang.String)>
Value Relationship Dependency,dfs.namenode.checkpoint.dir,hadoop.http.staticuser.user,org.apache.commons.lang.text.StrBuilder,<org.apache.commons.lang.text.StrBuilder: org.apache.commons.lang.text.StrBuilder ensureCapacity(int)>
Value Relationship Dependency,dfs.client.failover.sleep.base.millis,hadoop.kerberos.min.seconds.before.relogin,org.apache.hadoop.security.UserGroupInformation,"<org.apache.hadoop.security.UserGroupInformation: long getNextTgtRenewalTime(long,long,org.apache.hadoop.io.retry.RetryPolicy)>"
Value Relationship Dependency,dfs.ls.limit,hadoop.security.crypto.buffer.size,org.apache.hadoop.crypto.CryptoOutputStream,<org.apache.hadoop.crypto.CryptoOutputStream: void encrypt()>
Value Relationship Dependency,dfs.datanode.failed.volumes.tolerated,fs.protected.directories,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void <init>(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.DataStorage,org.apache.hadoop.conf.Configuration)>"
Value Relationship Dependency,fs.protected.directories,hadoop.security.groups.cache.secs,org.apache.hadoop.io.UTF8,<org.apache.hadoop.io.UTF8: void set(java.lang.String)>
Value Relationship Dependency,hadoop.http.staticuser.user,ipc.client.connect.timeout,org.apache.hadoop.net.NetUtils,"<org.apache.hadoop.net.NetUtils: void connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)>"
Value Relationship Dependency,dfs.user.home.dir.prefix,hadoop.http.staticuser.user,org.apache.hadoop.fs.Path,<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
Value Relationship Dependency,dfs.namenode.checkpoint.dir,io.file.buffer.size,org.apache.hadoop.hdfs.server.namenode.TransferFsImage,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler)>"
Value Relationship Dependency,spark.executor.cores,yarn.nodemanager.resource.cpu-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.executor.memory,yarn.nodemanager.resource.memory-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.executor.memory,yarn.scheduler.maximum-allocation-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.executor.memory,yarn.scheduler.minimum-allocation-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.executor.cores,yarn.scheduler.maximum-allocation-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.executor.cores,yarn.scheduler.minimum-allocation-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,yarn.app.mapreduce.am.resource.mb,yarn.scheduler.minimum-allocation-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,yarn.app.mapreduce.am.resource.mb,yarn.scheduler.maximum-allocation-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,yarn.app.mapreduce.am.resource.mb,yarn.nodemanager.resource.memory-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,yarn.app.mapreduce.am.resource.cpu-vcores,yarn.scheduler.minimum-allocation-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,yarn.app.mapreduce.am.resource.cpu-vcores,yarn.scheduler.maximum-allocation-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,yarn.app.mapreduce.am.resource.cpu-vcores,yarn.nodemanager.resource.cpu-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.driver.cores,yarn.nodemanager.resource.cpu-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.driver.cores,yarn.scheduler.maximum-allocation-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.driver.cores,yarn.scheduler.minimum-allocation-vcores,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.driver.memory,yarn.scheduler.minimum-allocation-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.driver.memory,yarn.scheduler.maximum-allocation-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,spark.driver.memory,yarn.nodemanager.resource.memory-mb,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils,"<org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource normalize(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>"
Value Relationship Dependency,mapreduce.reduce.memory.mb,yarn.nodemanager.resource.memory-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.reduce.cpu.vcores,yarn.nodemanager.resource.cpu-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.map.memory.mb,yarn.nodemanager.resource.memory-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.map.cpu.vcores,yarn.nodemanager.resource.cpu-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.reduce.cpu.vcores,yarn.scheduler.minimum-allocation-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.map.cpu.vcores,yarn.scheduler.maximum-allocation-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.reduce.cpu.vcores,yarn.scheduler.minimum-allocation-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.map.cpu.vcores,yarn.scheduler.maximum-allocation-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.reduce.memory.mb,yarn.scheduler.minimum-allocation-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.map.memory.mb,yarn.scheduler.minimum-allocation-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.reduce.memory.mb,yarn.scheduler.maximum-allocation-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,mapreduce.map.memory.mb,yarn.scheduler.maximum-allocation-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Value Relationship Dependency,alluxio.integration.master.resource.cpu,yarn.scheduler.minimum-allocation-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.api.ApplicationMasterProtocol: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
Value Relationship Dependency,alluxio.integration.worker.resource.cpu,yarn.scheduler.minimum-allocation-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.api.ApplicationMasterProtocol: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
Value Relationship Dependency,alluxio.integration.master.resource.mem,yarn.scheduler.minimum-allocation-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.api.ApplicationMasterProtocol: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
Value Relationship Dependency,alluxio.integration.worker.resource.mem,yarn.scheduler.minimum-allocation-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.api.ApplicationMasterProtocol: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
Value Relationship Dependency,alluxio.integration.master.resource.cpu,yarn.scheduler.maximum-allocation-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.api.ApplicationMasterProtocol: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
Value Relationship Dependency,alluxio.integration.worker.resource.cpu,yarn.scheduler.maximum-allocation-vcores,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.api.ApplicationMasterProtocol: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
Value Relationship Dependency,alluxio.integration.master.resource.mem,yarn.scheduler.maximum-allocation-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.api.ApplicationMasterProtocol: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
Value Relationship Dependency,alluxio.integration.worker.resource.mem,yarn.scheduler.maximum-allocation-mb,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor,<org.apache.hadoop.yarn.api.ApplicationMasterProtocol: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
Overwrite,maxSessionTimeout,zookeeper.session.timeout,org.apache.hadoop.hbase.zookeeper.HQuorumPeer,<org.apache.hadoop.hbase.zookeeper.HQuorumPeer: void writeMyID(java.util.Properties)>
Overwrite,dfs.data.transfer.protection,hadoop.rpc.protection,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil,<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.security.SaslPropertiesResolver getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)>
Overwrite,dfs.metrics.percentiles.intervals,hadoop.user.group.metrics.percentiles.intervals,org.apache.hadoop.hdfs.server.namenode.NameNode,<org.apache.hadoop.hdfs.server.namenode.NameNode: void initialize(org.apache.hadoop.conf.Configuration)>
Overwrite,hadoop.security.service.user.name.key,yarn.resourcemanager.principal,org.apache.hadoop.yarn.util.RMHAUtils,<org.apache.hadoop.yarn.util.RMHAUtils: org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getHAState(org.apache.hadoop.yarn.conf.YarnConfiguration)>
Overwrite,dfs.namenode.kerberos.principal,hadoop.security.service.user.name.key,org.apache.hadoop.hdfs.tools.DFSHAAdmin,<org.apache.hadoop.hdfs.tools.DFSHAAdmin: org.apache.hadoop.conf.Configuration addSecurityConfiguration(org.apache.hadoop.conf.Configuration)>
Overwrite,ha.failover-controller.graceful-fence.connection.retries,ipc.client.connect.max.retries,org.apache.hadoop.ha.FailoverController,"<org.apache.hadoop.ha.FailoverController: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)>"
Overwrite,ha.failover-controller.graceful-fence.connection.retries,ipc.client.connect.max.retries.on.timeouts,org.apache.hadoop.ha.FailoverController,"<org.apache.hadoop.ha.FailoverController: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)>"
Overwrite,hadoop.security.service.user.name.key,mapreduce.jobhistory.principal,org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin,<org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin: org.apache.hadoop.conf.Configuration addSecurityConfiguration(org.apache.hadoop.conf.Configuration)>
Overwrite,ipc.client.connect.max.retries,yarn.app.mapreduce.client-am.ipc.max-retries,org.apache.hadoop.mapred.ClientServiceDelegate,"<org.apache.hadoop.mapred.ClientServiceDelegate: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.ResourceMgrDelegate,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.v2.api.MRClientProtocol)>"
Overwrite,ipc.client.connect.max.retries.on.timeouts,yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts,org.apache.hadoop.mapred.ClientServiceDelegate,"<org.apache.hadoop.mapred.ClientServiceDelegate: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.ResourceMgrDelegate,org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.v2.api.MRClientProtocol)>"
Overwrite,ipc.client.connect.max.retries,yarn.client.failover-retries,org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider,"<org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider: void init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class)>"
Overwrite,ipc.client.connect.max.retries.on.timeouts,yarn.client.failover-retries-on-socket-timeouts,org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider,"<org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider: void init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class)>"
Overwrite,dfs.client.retry.policy.spec,yarn.node-labels.fs-store.retry-policy-spec,org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore,<org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore: void setFileSystem(org.apache.hadoop.conf.Configuration)>
Overwrite,dfs.client.retry.policy.spec,yarn.resourcemanager.fs.state-store.retry-policy-spec,org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore,<org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore: void startInternal()>
Overwrite,ipc.client.connect.max.retries.on.timeouts,yarn.resourcemanager.nodemanager-connect-retries,org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher,<org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher: void serviceInit(org.apache.hadoop.conf.Configuration)>
Overwrite,dfs.datanode.kerberos.principal,hadoop.security.service.user.name.key,org.apache.hadoop.hdfs.tools.DFSAdmin,<org.apache.hadoop.hdfs.tools.DFSAdmin: org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol getDataNodeProxy(java.lang.String)>
Default Value Dependency,dfs.client.read.shortcircuit.buffer.size,hbase.dfs.client.read.shortcircuit.buffer.size,org.apache.hadoop.hbase.util.CommonFSUtils,<org.apache.hadoop.hbase.util.CommonFSUtils: void checkShortCircuitReadBufferSize(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,mapreduce.reduce.shuffle.fetch.retry.enabled,yarn.nodemanager.recovery.enabled,org.apache.hadoop.mapreduce.task.reduce.Fetcher,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl,org.apache.hadoop.mapreduce.task.reduce.MergeManager,org.apache.hadoop.mapred.Reporter,org.apache.hadoop.mapreduce.task.reduce.ShuffleClientMetrics,org.apache.hadoop.mapreduce.task.reduce.ExceptionReporter,javax.crypto.SecretKey,int)>"
Default Value Dependency,dfs.data.transfer.saslproperties.resolver.class,hadoop.security.saslproperties.resolver.class,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil,<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.security.SaslPropertiesResolver getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,dfs.namenode.rpc-address,fs.defaultFS,org.apache.hadoop.hdfs.DFSUtil,<org.apache.hadoop.hdfs.DFSUtil: java.util.Map getNNServiceRpcAddresses(org.apache.hadoop.conf.Configuration)>
Default Value Dependency,hadoop.tmp.dir,yarn.resourcemanager.leveldb-state-store.path,*,*
Default Value Dependency,hadoop.tmp.dir,yarn.nodemanager.recovery.dir,*,*
Default Value Dependency,hadoop.tmp.dir,yarn.nodemanager.local-dirs,*,*
Default Value Dependency,hadoop.tmp.dir,yarn.resourcemanager.fs.state-store.uri,*,*
Default Value Dependency,hadoop.tmp.dir,yarn.timeline-service.leveldb-timeline-store.path,*,*
Default Value Dependency,hadoop.tmp.dir,yarn.timeline-service.leveldb-state-store.path,*,*
Default Value Dependency,hadoop.tmp.dir,yarn.scheduler.configuration.leveldb-store.path,*,*
Default Value Dependency,fs.s3.buffer.dir,hadoop.tmp.dir,*,*
Default Value Dependency,hadoop.tmp.dir,io.seqfile.local.dir,*,*
Default Value Dependency,fs.s3a.buffer.dir,hadoop.tmp.dir,*,*
Default Value Dependency,dfs.datanode.data.dir,hadoop.tmp.dir,*,*
Default Value Dependency,dfs.namenode.checkpoint.dir,hadoop.tmp.dir,*,*
Default Value Dependency,dfs.namenode.name.dir,hadoop.tmp.dir,*,*
Default Value Dependency,hadoop.tmp.dir,mapreduce.jobtracker.system.dir,*,*
Default Value Dependency,hadoop.tmp.dir,mapreduce.jobhistory.recovery.store.fs.uri,*,*
Default Value Dependency,hadoop.tmp.dir,mapreduce.cluster.local.dir,*,*
Default Value Dependency,hadoop.tmp.dir,mapreduce.cluster.temp.dir,*,*
Default Value Dependency,hadoop.tmp.dir,mapreduce.jobtracker.staging.root.dir,*,*
Default Value Dependency,hadoop.tmp.dir,mapreduce.jobhistory.recovery.store.leveldb.path,*,*
Default Value Dependency,fs.defaultFS,mapreduce.job.hdfs-servers,*,*
Default Value Dependency,mapreduce.jobhistory.intermediate-done-dir,yarn.app.mapreduce.am.staging-dir,*,*
Default Value Dependency,fs.local.block.size,mapreduce.job.ubertask.maxbytes,*,*
Behavior Dependency,hadoop.security.authentication,hbase.security.authentication,org.apache.hadoop.hbase.AuthUtil,<org.apache.hadoop.hbase.AuthUtil: org.apache.hadoop.hbase.ScheduledChore getAuthChore(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,dfs.internal.nameservices,fs.defaultFS,org.apache.hadoop.hbase.util.FSHDFSUtils,"<org.apache.hadoop.hbase.util.FSHDFSUtils: boolean isSameHdfs(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem)>"
Behavior Dependency,fs.defaultFS,hadoop.shell.missing.defaultFs.warning,org.apache.hadoop.fs.shell.FsCommand,<org.apache.hadoop.fs.shell.FsCommand: void processRawArguments(java.util.LinkedList)>
Behavior Dependency,hadoop.http.filter.initializers,yarn.timeline-service.http-cross-origin.enabled,org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer,<org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer: void startWebApp()>
Behavior Dependency,hadoop.http.filter.initializers,yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil: void setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager)>"
Behavior Dependency,hadoop.http.filter.initializers,yarn.resourcemanager.webapp.cross-origin.enabled,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil: void setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager)>"
Behavior Dependency,hadoop.http.authentication.type,yarn.resourcemanager.webapp.ui-actions.enabled,org.apache.hadoop.yarn.server.webapp.AppBlock,<org.apache.hadoop.yarn.server.webapp.AppBlock: void render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)>
Behavior Dependency,hadoop.security.authentication,yarn.timeline-service.enabled,org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl,"<org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl: void putTimelineDataInJSONFile(java.lang.String,java.lang.String)>"
Behavior Dependency,hadoop.security.authentication,yarn.log-aggregation-enable,org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl,<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: boolean isTokenKeepAliveEnabled(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,hadoop.security.authentication,yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil: void setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager)>"
Behavior Dependency,hadoop.security.authentication,yarn.resourcemanager.webapp.cross-origin.enabled,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil,"<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil: void setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager)>"
Behavior Dependency,hadoop.security.authentication,yarn.resourcemanager.webapp.ui-actions.enabled,org.apache.hadoop.yarn.server.webapp.AppBlock,<org.apache.hadoop.yarn.server.webapp.AppBlock: void render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)>
Behavior Dependency,dfs.metrics.percentiles.intervals,hadoop.user.group.metrics.percentiles.intervals,org.apache.hadoop.hdfs.server.namenode.NameNode,<org.apache.hadoop.hdfs.server.namenode.NameNode: void initialize(org.apache.hadoop.conf.Configuration)>
Behavior Dependency,dfs.permissions.enabled,hadoop.security.groups.cache.secs,org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp,"<org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp: void checkXAttrChangeAccess(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.fs.XAttr,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker)>"
Behavior Dependency,dfs.namenode.kerberos.principal,fs.defaultFS,org.apache.hadoop.security.SecurityUtil,"<org.apache.hadoop.security.SecurityUtil: java.lang.String replacePattern(java.lang.String[],java.lang.String)>"